Міністерство освіти і науки України
Харківський національний університет радіоелектроніки

Кафедра програмної інженерії

Звіт
до лабораторної роботи №4
з «Архітектура програмного забезпечення»

Виконав:

Перевірив:

ст. гр. ПЗПІ-22-9

ас. кафедри ПІ

Новомлинський В.І.

Дашенков Д.С.

Харків 2025

1 ІСТОРІЯ ЗМІН
Таблиця 1 - Історія змін
№

Дата

Версія звіту

Опис змін та
виправлень

1

04.06.2025

0.1

Створено звіт

2 ЗАВДАННЯ
Тема: Масштабування бекенду
В цій лабораторній роботі необхідно показати як можна масштабувати
бекенд системи для роботи із великим навантаженням. Для цього, можна на вибір:
масштабувати сервер горизонтально – багато копій сервера виконують однакові
функції для різних користувачів; масштабувати сервер вертикально – різні
мікросервіси виконують різні функції і масштабуються окремо одне від одного. На
найвищий бал на цю роботу необхідно провести навантажувальне тестування за
допомогою Gatling, JMeter, Locust чи іншого подібного інструмента і показати як зі
збільшенням кількості серверів зростає кількість запитів на секунду яку витримує
система.

3 ОПИС ВИКОНАНОЇ РОБОТИ
3.1 Стратегія масштабування серверної частини
У

межах

масштабування

бекенду

було

реалізовано

горизонтальне

масштабування серверної частини системи ArtGuard у середовищі Kubernetes. Для
цього створено об’єкт типу Deployment, у якому задається кількість реплік
(pod-ів), що паралельно обробляють вхідні HTTP-запити до серверної частини.
У

якості

стратегії

масштабування

застосовано

горизонтальне

масштабування, при якому кількість ідентичних pod-ів змінюється вручну за
потреби. Масштабування здійснювалося командою:
1

kubectl scale deployment artguard-deployment --replicas=N

Для маршрутизації зовнішніх запитів на pod-и застосовується об’єкт Service
з параметром type: NodePort (рис. 3.1), який відкриває певний порт на вузлі (ноді)
для доступу до застосунку ззовні. Таким чином, доступ до серверу здійснюється
через IP-адресу хоста Docker Desktop (наприклад, localhost) і призначений порт
(NodePort), а Kubernetes самостійно розподіляє трафік між активними подами в
межах цього сервісу.

Рисунок 3.1 – Використання NodePort-сервісу для зовнішнього доступу до подів
застосунку ArtGuard

3.2 Технічна реалізація масштабування
Технічна реалізація масштабування в системі ArtGuard базується на
використанні контейнеризації Docker та оркестрації Kubernetes. Серверна частина
написана мовою Go, побудована за принципами REST API та не зберігає стан між
запитами, що дозволяє легко створювати декілька ідентичних інстансів застосунку.
Основні технічні рішення:
-​ Deployment у Kubernetes відповідає за створення та управління подами
(екземплярами серверу). Зміна параметра replicas дозволяє збільшити або
зменшити кількість активних pod-ів без зміни коду.
-​ Service типу NodePort надає можливість зовнішнього доступу до застосунку.
Він відкриває порт на ноді (хості), за яким клієнти можуть надсилати
HTTP-запити, а Kubernetes автоматично перенаправляє їх на один із
доступних pod-ів. Така конфігурація дозволяє тестувати масштабування
навіть у середовищі Docker Desktop без хмарної інфраструктури.
-​ База даних PostgreSQL працює в окремому pod-і, який має постійне
сховище, описане через PersistentVolumeClaim. Завдяки цьому, незалежно від
кількості pod-ів бекенду, усі вони підключаються до однієї централізованої
бази, що забезпечує узгодженість даних.
-​ Змінні оточення (наприклад, логін і пароль до БД) передаються кожному
pod-у через параметр env у YAML-файлі deployment.yaml. Це дозволяє
зберігати конфігурацію централізовано.
-​ Docker-образ бекенду створюється з власного Dockerfile, де вказується версія
Go, необхідні залежності, та компіляція головного файлу main.go.

Рисунок 3.2 – Контейнери в Docker Desktop
3.3 Опис навантажувальних тестів
Для перевірки масштабованості та продуктивності системи ArtGuard було
проведено серію навантажувальних тестів за допомогою інструмента Locust —
сучасного open-source фреймворку для створення сценаріїв навантаження
HTTP-запитами.
Тестування проводилося на типовому маршруті /api/objects, який є
публічним та повертає перелік об’єктів системи. Цей ендпоінт обрано, оскільки
він не вимагає авторизації й репрезентує типову взаємодію з API з боку клієнта.
У процесі тестування були симульовані три сценарії із різною кількістю
pod-ів:
-​ 1 pod (початковий варіант);
-​ 2 pod-и (після масштабування);
-​ 3 pod-и (максимальна конфігурація у рамках роботи).
У кожному тесті Locust запускав 1000 одночасних користувачів зі швидкістю
появи 25 користувачів на секунду. Тривалість кожного тесту становила в
середньому 3 хвилини.
Цілі тестування:
-​ перевірити, як система реагує на зростання навантаження;
-​ оцінити зміни в часі відповіді (latency);

-​ виміряти кількість запитів на секунду (RPS);
-​ визначити наявність або відсутність помилок.
Для кожного сценарію були зафіксовані:
-​ статистика запитів (середня/максимальна затримка, кількість помилок);
-​ графіки завантаження з вкладки Charts в інтерфейсі Locust.

Рисунок 3.3 – Конфігурація запуску тесту в Locust

Рисунок 3.4– Результат тестування системи на 1-му поді

Рисунок 3.5 – Результат тестування системи на 1-му поді

Рисунок 3.6 – Результат тестування системи на 2-ох подах

Рисунок 3.6 – Результат тестування системи на 2-ох подах

Результат тестування системи на 3-ох подах

Результат тестування системи на 3-ох подах
3.4 Аналіз результатів тесту
pods RPS

Avg (мс) Max (мс) Median 95-й
перцент
иль

99-й
Помилки
перцентиль

1

660.7 4.04

85

3

8

20

0%

2

659.4 3.93

102

3

8

20

0%

3

665.2 3.65

69

3

7

16

0%

1 pod - Стабільна робота, але відчутна латентність.
2 pods - Латентність трохи нижча, RPS стабільний.
3 pods - Найкраща стабільність, зменшення затримки, RPS трохи зріс.

Підсумок:
-​ Система горизонтально масштабується, але не лінійно — RPS не зростає
сильно через обмеження бази даних.
-​ Проте затримки зменшуються, а стабільність зберігається — це свідчить про
ефективне балансування навантаження Kubernetes.
-​ RPS виріс із 660.7 до 665.2, але це мінімальне зростання говорить, що
вузьким місцем стає PostgreSQL pod або ресурси хоста (наприклад, Docker
Desktop).
​

3.5 Аналіз вузьких місць
Під час навантажувального тестування було виявлено, що зі збільшенням

кількості pod-ів серверної частини приріст продуктивності є помірним. Це
свідчить про наявність вузького місця в архітектурі.
Основним обмеженням виступає централізована база даних PostgreSQL, яка
розгорнута в одному pod-і. Усі pod-и бекенду одночасно підключаються до цієї БД,
що створює велику кількість паралельних з'єднань. При високому навантаженні це
призводить до підвищення затримок та зниження загальної ефективності
масштабування.
Крім того, тестування проводилось у локальному середовищі Docker
Desktop, яке має обмеження по ресурсах (CPU, RAM). Це також могло вплинути на
результати при запуску кількох pod-ів і бази на одному хості.
Таким чином, вузьким місцем у поточній реалізації є база даних та
обмеження локального середовища. У реальних умовах хмарної інфраструктури ці
обмеження можна зменшити за рахунок горизонтального масштабування БД або
використання managed-сервісів.

Посилання на github:
https://github.com/NureNovomlynskyiViktor/apz-pzpi-22-9-novomlynskyi-viktor/tree/m
ain/Lab4

4 ВИСНОВОК
У результаті проведених навантажувальних тестів було підтверджено, що
система підтримує горизонтальне масштабування за рахунок збільшення кількості
pod-ів у Kubernetes. Було проведено три окремі сесії тестування з однаковим
навантаженням у 1000 одночасних користувачів: при одному, двох та трьох pod-ах,
які обслуговують серверну частину. У всіх випадках система демонструвала
стабільну роботу без помилок, що свідчить про її високу надійність.
При збільшенні кількості pod-ів спостерігалося поступове покращення
продуктивності: середній час відповіді знизився з 4.04 мс (1 pod) до 3.65 мс (3
pod-и), а максимальні затримки також зменшились — з 85 мс до 69 мс. Хоча
загальний показник RPS залишився майже незмінним (660–665), це вказує на те,
що вузьким місцем, ймовірно, виступає база даних або апаратні ресурси
хост-машини (Docker Desktop).
Загалом, результати показали, що система добре масштабується
горизонтально, а Kubernetes LoadBalancer ефективно розподіляє навантаження між
подами. Це підтверджує доцільність обраної архітектури для роботи з високим
рівнем паралелізму запитів у реальних умовах експлуатації.

